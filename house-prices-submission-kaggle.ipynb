{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Academic Integrity and Learning Statement\n",
    "\n",
    "By submitting my work, I confirm that:\n",
    "\n",
    "1. The code, analysis, and documentation in this notebook are my own work and reflect my own understanding.\n",
    "2. I am prepared to explain all code and analysis included in this submission.\n",
    "\n",
    "If I used assistance (e.g., AI tools, tutors, or other resources), I have:\n",
    "\n",
    "- Clearly documented where and how external tools or resources were used in my solution.\n",
    "- Included a copy of the interaction (e.g., AI conversation or tutoring notes) in an appendix.\n",
    "\n",
    "I acknowledge that:\n",
    "\n",
    "- I may be asked to explain any part of my code or analysis during evaluation.\n",
    "- Misrepresenting assisted work as my own constitutes academic dishonesty and undermines my learning."
   ],
   "id": "71e8cdf0751690d6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-04T05:27:03.075629Z",
     "start_time": "2025-07-04T05:27:02.949358Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "\n",
    "import comet_ml, mlflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import platform\n",
    "from joblib import load"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Enable auto-reload extension\n",
    "%load_ext autoreload\n",
    "# Automatically reload all modules before executing code\n",
    "%autoreload 2"
   ],
   "id": "c46da9c6fd73de53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import base_utils_logging\n",
    "import proj_utils_data_loader\n",
    "import proj_configs\n",
    "import proj_utils\n",
    "import proj_utils_feat_engg\n",
    "import proj_utils_plots\n",
    "import proj_utils_model"
   ],
   "id": "88ba6ac1d373ffce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")"
   ],
   "id": "f5129035565a3994",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check software specs\n",
    "dict_sw_version = {\n",
    "    'python': os.popen('python --version').read().strip(),\n",
    "    'numpy': np.__version__,\n",
    "    'pandas': pd.__version__,\n",
    "    'optuna': optuna.__version__,\n",
    "    'mlflow': mlflow.__version__,\n",
    "}\n",
    "\n",
    "for key, value in dict_sw_version.items():\n",
    "    print(f'{proj_utils_plots.beautify(key, 1)} version is: {proj_utils_plots.beautify(value)}')\n"
   ],
   "id": "6d24b286ccfc7d22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check hardware specs\n",
    "def get_mac_gpu_info():\n",
    "    try:\n",
    "        # Get system information about GPU\n",
    "        result = subprocess.run(['system_profiler', 'SPDisplaysDataType'],\n",
    "                              capture_output=True, text=True)\n",
    "        return result.stdout\n",
    "    except Exception as e:\n",
    "        return f\"Error getting GPU info: {e}\"\n",
    "\n",
    "# Check CPU cores\n",
    "print(f'CPU cores available to use: {proj_utils_plots.beautify(str(multiprocessing.cpu_count()))}')\n",
    "\n",
    "# Check MPS availability\n",
    "print(\"TensorFlow GPU devices:\", proj_utils_plots.beautify(tf.config.list_physical_devices('GPU')))\n",
    "print(f\"Processor: {proj_utils_plots.beautify(platform.processor())}\")\n",
    "print(f\"Machine: {proj_utils_plots.beautify(platform.machine())}\")\n",
    "\n",
    "print(\"PyTorch MPS (Metal) Status:\")\n",
    "print(f\"MPS available: {proj_utils_plots.beautify(torch.backends.mps.is_available())}\")\n",
    "print(f\"MPS built: {proj_utils_plots.beautify(str(torch.backends.mps.is_built()))}\")\n",
    "\n",
    "# Get detailed GPU information\n",
    "print(\"\\nDetailed GPU Information:\")\n",
    "print(get_mac_gpu_info())"
   ],
   "id": "337330b589c6a835",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_utils_logging.setup_logging()",
   "id": "a31b0f710678697c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "base_utils_logging.logger.info('Starting the application')",
   "id": "4e9d276c1b24d439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_raw_train = proj_utils_data_loader.load_data(proj_configs.TRAIN_FILE)\n",
    "df_raw_test = proj_utils_data_loader.load_data(proj_configs.TEST_FILE)\n",
    "df_raw_train.shape, df_raw_test.shape"
   ],
   "id": "eed9d9da9617bed0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_raw_train.sample(3)",
   "id": "d3f3a25cad6101e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: uncomment and comment below insignificant col list\n",
    "# insignificant_cols = ['Order', 'PID']\n",
    "insignificant_cols = ['Id']\n",
    "target_col = 'SalePrice'\n",
    "ignorables_cols = insignificant_cols + [target_col]\n",
    "ordinal_cols = ['LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "temporal_cols_name_pattern = ['Yr', 'Year']"
   ],
   "id": "6edf5a9a26a9e843",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_raw_all, df_raw_target = proj_utils_data_loader.merge_train_test_data(df_raw_train, df_raw_test, insignificant_cols, target_col)\n",
    "df_raw_all.shape, df_raw_target.shape"
   ],
   "id": "654f96a21abe775e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Slice of train data records from the merged data frame\n",
    "df_train = df_raw_all[df_raw_all['is_train']==1].iloc[:,:-1]"
   ],
   "id": "e2bde6206601a714",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "23d92e996dde6775",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_cat_cardinality_threshold = proj_configs.CATEGORICAL_CARDINALITY_THRESHOLD_ABS\n",
    "threshold_type = 'ABS'\n",
    "feature_categories = proj_utils_feat_engg.classify_columns(df=df_train, n_cat_threshold=n_cat_cardinality_threshold, threshold_type=threshold_type, cols_to_ignore=ignorables_cols, temporal_cols_name_pattern=temporal_cols_name_pattern, ordinal_cols=ordinal_cols)"
   ],
   "id": "a7a54181e30f958",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols_num_continuous, n_num_continuous, cols_num_discrete, n_num_discrete, cols_cat_nominal, n_cat_nominal, cols_cat_ordinal, n_cat_ordinal, cols_object, n_object, cols_temporal, n_temporal, cols_binary, n_binary = proj_utils_feat_engg.get_cols_as_tuple(feature_categories)\n",
    "\n",
    "n_total = df_train.shape[1] - len(ignorables_cols)\n",
    "\n",
    "print(f\"=\"*80)\n",
    "print(f\"Total raw columns = {proj_utils_plots.beautify(str(len(df_train.columns)))} \\nNumerical Continuous = {proj_utils_plots.beautify(n_num_continuous)} \\nNumerical Discrete = {proj_utils_plots.beautify(n_num_discrete)} \\nCategorical Nominal = {proj_utils_plots.beautify(n_cat_nominal)} \\nCategorical Ordinal = {proj_utils_plots.beautify(n_cat_ordinal)} \\nObject/String = {proj_utils_plots.beautify(n_object)} \\nTemporal = {proj_utils_plots.beautify(n_temporal)} \\nBinary = {proj_utils_plots.beautify(n_binary)}\")\n",
    "\n",
    "print(f\"=\"*80)\n",
    "print(f\"Any inconsistencies detected?[True/False] = {proj_utils_plots.beautify('True', 3) if n_total != len(df_train.columns) - len(ignorables_cols) else proj_utils_plots.beautify('False', 1)}\")\n",
    "print(f'='*80)"
   ],
   "id": "5f5c9dd9c6a705f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the number of NaN values for each column\n",
    "nan_counts = df_train.isna().sum()\n",
    "\n",
    "# Filter only columns that have NaN values and sort by the number of NaNs\n",
    "cols_with_nans = nan_counts[nan_counts > 0].index.tolist()\n",
    "print(f\"Columns with NaNs: = {proj_utils_plots.beautify(str(len(cols_with_nans)))}/{proj_utils_plots.beautify(n_total)}\")\n",
    "print(f\"And they are: {cols_with_nans}\")"
   ],
   "id": "37ae4437ee9b5313",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_cardinality = proj_utils_feat_engg.get_cardinality_df(df_train)",
   "id": "11e8333c6ef675a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_cardinality",
   "id": "53b28a112abc4332",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "proj_utils_plots.plot_cardinality(df_cardinality, n_cat_cardinality_threshold, threshold_used=threshold_type, type_of_cols='all', figsize=(20, 6))",
   "id": "804341bb4547354a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Zoom into cols having NaNs only\n",
    "proj_utils_plots.plot_cardinality(df_cardinality[df_cardinality['col_name'].isin(cols_with_nans)], n_cat_cardinality_threshold, threshold_used=threshold_type, type_of_cols=\"NaN\", figsize=(10, 6))"
   ],
   "id": "e2e3347129e57abc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_train[cols_num_continuous].isnull().sum().sort_values(ascending=False)",
   "id": "5a29edbf7dd56d0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creating a copy of the raw data to impute missing values for plotting purposes only (as NaNs are not plotted)\n",
    "df_imputed_for_plots = df_train.copy()\n",
    "df_imputed_for_plots[cols_num_continuous] = df_imputed_for_plots[cols_num_continuous].fillna(0)\n",
    "most_frequent = df_imputed_for_plots[cols_num_discrete].mode().iloc[0]\n",
    "df_imputed_for_plots[cols_num_discrete] = df_imputed_for_plots[cols_num_discrete].fillna(most_frequent)"
   ],
   "id": "ef2ca147da0befc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_imputed_for_plots",
   "id": "ec2adcd985825dc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_raw_target",
   "id": "b1aa4286799fc93c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_imputed_for_plots_v2 = pd.concat([df_train[cols_num_continuous], df_raw_target], axis=1)",
   "id": "b5bf19409fae99b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_imputed_for_plots_v2.sample(2)",
   "id": "891c743a604d4373",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# correlation_plot = plot_correlation_with_demand(df, save_path=\"correlation_plot.png\")\n",
    "correlation_plot = proj_utils_plots.plot_correlation_with_target(df_imputed_for_plots_v2, target_col)"
   ],
   "id": "6e372acf601b0ea0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "proj_utils_plots.plot_numerical_distribution(df_imputed_for_plots, cols_num_continuous)",
   "id": "6e705f9717f2acaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "proj_utils_plots.plot_categorical_distribution(df_imputed_for_plots, cols_cat_nominal)",
   "id": "b0482db21836bfd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "proj_utils_plots.plot_categorical_distribution(df_imputed_for_plots, cols_cat_ordinal)",
   "id": "5cc64fc1e5fa526",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "proj_utils_plots.plot_relationship_to_target(df_imputed_for_plots, cols_num_discrete, target_col)",
   "id": "565be807881cf88d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "proj_utils_plots.plot_relationship_to_target(df_imputed_for_plots, cols_num_discrete, target_col, trend_type='median')",
   "id": "34e212dc79d02e14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df_train,\n",
    "    df_raw_target,\n",
    "    test_size=proj_configs.VALIDATION_SIZE,\n",
    "    random_state=proj_configs.RANDOM_STATE\n",
    ")"
   ],
   "id": "57651d20d7b35ba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train.shape, y_train.shape, X_val.shape, y_val.shape",
   "id": "40539573c8a9ad71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_columns = cols_num_continuous\n",
    "cat_columns = cols_cat_nominal + cols_cat_ordinal + cols_num_discrete + cols_binary + cols_object\n",
    "tempo_columns = cols_temporal"
   ],
   "id": "26440addd4e93771",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(num_columns), len(cat_columns), len(tempo_columns)",
   "id": "1847e20decebb02e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pproc_pipe = proj_utils_feat_engg.create_pproc_pipeline(num_columns, cat_columns, tempo_columns)",
   "id": "8bf6e9c951bd6c76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "daa38382de0ade53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Login to mlflow\n",
    "# mlflow.login()\n",
    "# proj_utils_model.set_mlflow_uri(\"databricks\")\n",
    "# mlflow_experiment_name = f\"/Users/asheesh.ambardar@live.com/{proj_configs.PROJECT_NAME}\"\n",
    "# mlflow_experiment_id = proj_utils_model.get_or_create_experiment(mlflow_experiment_name)\n",
    "# proj_utils_model.set_mlflow_experiment(mlflow_experiment_name)"
   ],
   "id": "179433c870506e32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comet_experiment = comet_ml.Experiment()\n",
    "# comet_experiment.set_name(proj_configs.PROJECT_NAME)"
   ],
   "id": "fe076592ea14ce7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verify connection\n",
    "if comet_experiment.api_key:\n",
    "    print(\"Successfully connected to Comet ML!\")\n",
    "else:\n",
    "    print(\"Failed to connect to Comet ML\")\n"
   ],
   "id": "f8cdb11560d30c5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "82f483bc1028840a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "19c88e6151c92703",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b35f48c37d77d9a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "aab859eeded8821f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "49af076af4d9f832",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings, logging\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "logging.getLogger(\"mlflow\").setLevel(logging.ERROR)"
   ],
   "id": "fc8e596f1be235",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_train_transformed = pproc_pipe.fit_transform(X_train)",
   "id": "39926242db7dc827",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X_val_transformed = pproc_pipe.transform(X_val)",
   "id": "75e65af0987fae80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_train_transformed = y_train.to_numpy()\n",
    "y_val_transformed = y_val.to_numpy()"
   ],
   "id": "d0d39608468e826f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for both NaN and None\n",
    "has_nulls_or_nans = pd.isna(X_train_transformed).any()\n",
    "print(f\"Contains null or NaN values: {has_nulls_or_nans}\")"
   ],
   "id": "9169473752777ba1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(y_val)",
   "id": "7bf1cfaa4dd9cdc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "comet_experiment = comet_ml.Experiment()\n",
    "run_name='xgb-07'\n",
    "try:\n",
    "    optimised_study_xgb = proj_utils_model.run_hyperparam_tuning_xgb_exp(X_train_transformed, y_train_transformed, X_val_transformed, y_val_transformed, comet_experiment, run_name, proj_configs.OPTUNA_TRIAL_COUNT)\n",
    "finally:\n",
    "    comet_experiment.end()"
   ],
   "id": "7ccb415827288759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e87c532874678e14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "107c89f0b327c6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name='lasso-03'\n",
    "artefact_path = 'artefact_path'\n",
    "optimised_study_lasso = proj_utils_model.run_hyperparam_tuning_lasso(X_train, y_train, X_val, y_val, pproc_pipe, mlflow_experiment_id, run_name, artefact_path, proj_configs.OPTUNA_TRIAL_COUNT)"
   ],
   "id": "a11b7ce0f2cba7a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name='xgb-02'\n",
    "artefact_path = 'artefact_path'\n",
    "optimised_study_xgb = proj_utils_model.run_hyperparam_tuning_xgb(X_train_transformed, y_train_transformed, X_val_transformed, y_val_transformed, mlflow_experiment_id, run_name, artefact_path, proj_configs.OPTUNA_TRIAL_COUNT)"
   ],
   "id": "e164911336104924",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "run_name='rfc-05'\n",
    "artefact_path = 'artefact_path'\n",
    "optimised_study_rfc = proj_utils_model.run_hyperparam_tuning_rfc(X_train, y_train, X_val, y_val, pproc_pipe, mlflow_experiment_id, run_name, artefact_path, proj_configs.OPTUNA_TRIAL_COUNT)"
   ],
   "id": "b99716eff05af8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "58fd387356b54f49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T05:29:22.138278Z",
     "start_time": "2025-07-04T05:29:22.044748Z"
    }
   },
   "cell_type": "code",
   "source": "print(os.path.exists(f\"{proj_configs.PATH_OUT_MODELS}model.pkl\"))",
   "id": "36c1a316e6b1b48b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T05:30:01.347999Z",
     "start_time": "2025-07-04T05:30:01.293012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def safe_load_model(model_path):\n",
    "    try:\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        return model\n",
    "    except (pickle.UnpicklingError, KeyError) as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        print(\"This might be due to version mismatch or corrupted file\")\n",
    "        return None\n",
    "\n",
    "# Load the model\n",
    "model_path = f\"{proj_configs.PATH_OUT_MODELS}model.pkl\"\n",
    "loaded_model = safe_load_model(model_path)\n",
    "\n",
    "if loaded_model is not None:\n",
    "    print(\"Model loaded successfully\")"
   ],
   "id": "d50a7addf30bb5f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: invalid load key, '{'.\n",
      "This might be due to version mismatch or corrupted file\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T05:27:42.856348Z",
     "start_time": "2025-07-04T05:27:42.586474Z"
    }
   },
   "cell_type": "code",
   "source": "loaded_model = load(f\"{proj_configs.PATH_OUT_MODELS}model.pkl\")",
   "id": "e4cb153a38d097af",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "123",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m loaded_model \u001B[38;5;241m=\u001B[39m load(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mproj_configs\u001B[38;5;241m.\u001B[39mPATH_OUT_MODELS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mmodel.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/my_py311_env/lib/python3.11/site-packages/joblib/numpy_pickle.py:658\u001B[0m, in \u001B[0;36mload\u001B[0;34m(filename, mmap_mode)\u001B[0m\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fobj, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    653\u001B[0m                 \u001B[38;5;66;03m# if the returned file object is a string, this means we\u001B[39;00m\n\u001B[1;32m    654\u001B[0m                 \u001B[38;5;66;03m# try to load a pickle file generated with an version of\u001B[39;00m\n\u001B[1;32m    655\u001B[0m                 \u001B[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001B[39;00m\n\u001B[1;32m    656\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m load_compatibility(fobj)\n\u001B[0;32m--> 658\u001B[0m             obj \u001B[38;5;241m=\u001B[39m _unpickle(fobj, filename, mmap_mode)\n\u001B[1;32m    659\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "File \u001B[0;32m/opt/anaconda3/envs/my_py311_env/lib/python3.11/site-packages/joblib/numpy_pickle.py:577\u001B[0m, in \u001B[0;36m_unpickle\u001B[0;34m(fobj, filename, mmap_mode)\u001B[0m\n\u001B[1;32m    575\u001B[0m obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 577\u001B[0m     obj \u001B[38;5;241m=\u001B[39m unpickler\u001B[38;5;241m.\u001B[39mload()\n\u001B[1;32m    578\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m unpickler\u001B[38;5;241m.\u001B[39mcompat_mode:\n\u001B[1;32m    579\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe file \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has been generated with a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    580\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjoblib version less than 0.10. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    581\u001B[0m                       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease regenerate this pickle file.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    582\u001B[0m                       \u001B[38;5;241m%\u001B[39m filename,\n\u001B[1;32m    583\u001B[0m                       \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/my_py311_env/lib/python3.11/pickle.py:1213\u001B[0m, in \u001B[0;36m_Unpickler.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1211\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m\n\u001B[1;32m   1212\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, bytes_types)\n\u001B[0;32m-> 1213\u001B[0m         dispatch[key[\u001B[38;5;241m0\u001B[39m]](\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _Stop \u001B[38;5;28;01mas\u001B[39;00m stopinst:\n\u001B[1;32m   1215\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m stopinst\u001B[38;5;241m.\u001B[39mvalue\n",
      "\u001B[0;31mKeyError\u001B[0m: 123"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T05:18:39.200606Z",
     "start_time": "2025-07-04T05:18:39.083176Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bba5b6aa566cedbe",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Experiment' object has no attribute 'get_model_registry'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# First, get the model registry\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m model_registry \u001B[38;5;241m=\u001B[39m comet_experiment\u001B[38;5;241m.\u001B[39mget_model_registry()\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Then get the model\u001B[39;00m\n\u001B[1;32m      5\u001B[0m loaded_model \u001B[38;5;241m=\u001B[39m model_registry\u001B[38;5;241m.\u001B[39mget_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxgb_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Experiment' object has no attribute 'get_model_registry'"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-04T05:15:53.762580Z",
     "start_time": "2025-07-04T05:15:53.624876Z"
    }
   },
   "cell_type": "code",
   "source": "loaded_model = comet_experiment.get_model(\"xgb_model\")  # The name you used when logging\n",
   "id": "575da2c882629d2c",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Experiment' object has no attribute 'get_model'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m loaded_model \u001B[38;5;241m=\u001B[39m comet_experiment\u001B[38;5;241m.\u001B[39mget_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxgb_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Experiment' object has no attribute 'get_model'"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_uri = mlflow.get_artifact_uri(artefact_path)\n",
    "model_uri"
   ],
   "id": "f9601a04226dce96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loaded_model = mlflow.sklearn.load_model(\n",
    "    model_uri=\"dbfs:/databricks/mlflow-tracking/1539464224853128/d72a53fe0a31467084ebeaceb1edc48e/artifacts/artefact_path\"\n",
    ")"
   ],
   "id": "2b1ef5dd7c0def75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "loaded_model",
   "id": "c2dfeac181ec5d64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_raw_test = proj_utils_data_loader.load_data(proj_configs.TEST_FILE)",
   "id": "f9813d5c1c2f894f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_test = proj_utils_data_loader.refactor_col_names(df_raw_test)",
   "id": "def8103b96c87508",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_test.sample(3)",
   "id": "a8565fbd09eb96f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_test.drop(insignificant_cols, axis=1, inplace=True)",
   "id": "f1c98123876eafa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_test_transformed = pproc_pipe.transform(df_test)\n",
    "type(data_test_transformed)"
   ],
   "id": "238e2c79243696a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for both NaN and None\n",
    "has_nulls_or_nans = pd.isna(df_test_transformed).any()\n",
    "print(f\"Contains null or NaN values: {has_nulls_or_nans}\")"
   ],
   "id": "3ee508e6c4787cd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_raw_train = df_raw.copy()\n",
    "df_raw_train.drop(ignorables_cols, axis=1, inplace=True)"
   ],
   "id": "9ed7501cd4edf825",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_train_transformed = pproc_pipe.transform(df_raw_train)\n",
    "type(data_train_transformed)"
   ],
   "id": "7f592343c68b691f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for both NaN and None\n",
    "has_nulls_or_nans = pd.isna(df_train_transformed).any()\n",
    "print(f\"Contains null or NaN values: {has_nulls_or_nans}\")"
   ],
   "id": "af40d43c07e572d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_target = df_raw[target_col].to_numpy()\n",
    "type(data_target)"
   ],
   "id": "1ec822cb82024ada",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "loaded_model.fit(data_train_transformed, data_target)",
   "id": "be9247fae05618e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_preds = loaded_model.predict(data_train_transformed)\n",
    "train_actuals = data_target"
   ],
   "id": "8e369c283caeff2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "train_mse = mean_squared_error(train_actuals, train_preds).round(5)\n",
    "train_r2 = r2_score(train_actuals, train_preds).round(5)\n",
    "\n",
    "print(\"=== Model Performance ===\")\n",
    "print(f\"Train MSE: {proj_utils_plots.beautify(train_mse)}, Train R2: {proj_utils_plots.beautify(train_r2)}\")"
   ],
   "id": "b8c7ae9eaa5ce6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_preds = loaded_model.predict(data_test_transformed)",
   "id": "3b9b0432f7f6176b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_preds",
   "id": "359858a87b51c212",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "my_submission = pd.DataFrame({'Id': df_raw_test.Id, 'SalePrice': test_preds})\n",
    "# you could use any filename. We choose submission here\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ],
   "id": "ace23632daf3e027",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "19b99d11f56ec07f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "31dd1df29e5ced5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "482be93cb73a5911",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this is a Trial object, not the underlying ML object.\n",
    "best_performing_trial = optimised_study_xgb.best_trial\n",
    "print(f'Best trial was at number {proj_utils_plots.beautify(str(best_performing_trial.number), 1)} with params as:\\n {proj_utils_plots.beautify(str(best_performing_trial.params), 2)}')\n",
    "print(f'Best score value is: {proj_utils_plots.beautify(str(best_performing_trial.value))}')"
   ],
   "id": "4a77b77c006c759a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_performing_trial",
   "id": "31d846d4b65b7a22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "study_full_metrics = optimised_study_xgb.trials_dataframe()\n",
    "# save the metrics to a file\n",
    "proj_utils_model.save_hyperparams(f'full_metrics_{proj_utils.get_current_timestamp()}.csv', proj_configs.PATH_OUT_MODELS, study_full_metrics)\n",
    "\n",
    "# peek at the full metrics dataframe\n",
    "study_full_metrics"
   ],
   "id": "657b2c2f510cd79a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fetch number of trial runs per model type\n",
    "num_lr_trials = study_full_metrics[study_full_metrics['params_model'] == 'lr'].shape[0]\n",
    "num_lasso_trials = study_full_metrics[study_full_metrics['params_model'] == 'lasso'].shape[0]\n",
    "num_ridge_trials = study_full_metrics[study_full_metrics['params_model'] == 'ridge'].shape[0]\n",
    "num_elasticnet_trials = study_full_metrics[study_full_metrics['params_model'] == 'elasticnet'].shape[0]\n",
    "\n",
    "print(f'Total trials = {proj_utils_plots.beautify(str(num_lr_trials + num_lasso_trials + num_ridge_trials + num_elasticnet_trials), 1)}\\n-- LR trials = {proj_utils_plots.beautify(str(num_lr_trials), 1)}\\n-- Lasso trials = {proj_utils_plots.beautify(str(num_lasso_trials), 1)}\\n-- Ridge trials = {proj_utils_plots.beautify(str(num_ridge_trials), 1)}\\n-- ElasticNet trials = {proj_utils_plots.beautify(str(num_elasticnet_trials), 1)}')"
   ],
   "id": "2c629463bc37a59b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# retrieve all performance values for each model type studied\n",
    "grp_by_model_type_val = study_full_metrics.groupby('params_model')['value'].apply(list)\n",
    "# retrieve the best performing model (use nsmallest if Optuna objective was to minimise,\n",
    "grp_by_model_type_best_val = study_full_metrics.groupby('params_model')['value'].nsmallest(1)\n",
    "# display the stats\n",
    "grp_by_model_type_best_val"
   ],
   "id": "455d563c4610f4d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " # grouping Optuna metrics by model type and using idxmax (or idmin) method to find a row with the best model performance (value) for each group\n",
    "study_best_model_group = study_full_metrics.loc[study_full_metrics.groupby('params_model')['value'].idxmin()]"
   ],
   "id": "30208428b7d90d20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "study_best_model_group",
   "id": "d2f168858eabbdf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# retrieve the trial number of the best model for each model type - the Optuna metrics dataframe index and trial number are the same.\n",
    "best_lr_trial = study_best_model_group[study_best_model_group['params_model'] == 'lr']['number'].values[0]\n",
    "best_lasso_trial = study_best_model_group[study_best_model_group['params_model'] == 'lasso']['number'].values[0]\n",
    "best_ridge_trial = study_best_model_group[study_best_model_group['params_model'] == 'ridge']['number'].values[0]\n",
    "best_elasticnet_trial = study_best_model_group[study_best_model_group['params_model'] == 'elasticnet']['number'].values[0]\n",
    "\n",
    "final_pipe_best_lr = models[best_lr_trial]\n",
    "best_model_lr = final_pipe_best_lr.named_steps['regressor']\n",
    "final_pipe_best_lasso = models[best_lasso_trial]\n",
    "best_model_lasso = final_pipe_best_lasso.named_steps['regressor']\n",
    "final_pipe_best_ridge = models[best_ridge_trial]\n",
    "best_model_ridge = final_pipe_best_ridge.named_steps['regressor']\n",
    "final_pipe_best_elasticnet = models[best_elasticnet_trial]\n",
    "best_model_elasticnet = final_pipe_best_elasticnet.named_steps['regressor']\n",
    "\n",
    "# retrieve the best model object (amongst all model types evaluated)\n",
    "final_pipe_best = models[best_performing_trial.number]\n",
    "best_model = final_pipe_best.named_steps['regressor']"
   ],
   "id": "6233d4364e331135",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_pipe_best",
   "id": "f7a2a5cb76f9512d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "final_pipe_best.fit(X_train, y_train)",
   "id": "a7fc4a11b6e8b14e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cols_final_inputs, cols_final_output_features = proj_utils_feat_engg.get_final_features(final_pipe_best, X_train)",
   "id": "38d1696dc80f1fdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "proj_utils_model.save_features(f'pproc_final_input_cols_{len(cols_final_inputs)}_{proj_utils.get_current_timestamp()}.csv', proj_configs.PATH_OUT_FEATURES, pd.DataFrame(cols_final_inputs))\n",
    "proj_utils_model.save_features(f'pproc_final_output_features_{len(cols_final_output_features)}_{proj_utils.get_current_timestamp()}.csv', proj_configs.PATH_OUT_FEATURES, pd.DataFrame(cols_final_output_features))"
   ],
   "id": "a22159b4cc78d7af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "proj_utils_model.save_model(f'final_pipe_{proj_utils.get_current_timestamp()}.pkl', proj_configs.PATH_OUT_MODELS, final_pipe_best)",
   "id": "65b972ae40e6a624",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_train_preds = final_pipe_best.predict(X_train)\n",
    "y_val_preds = final_pipe_best.predict(X_val)"
   ],
   "id": "bff38c8eaa4fe415",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model\n",
    "train_mse = mean_squared_error(y_train, y_train_preds).round(5)\n",
    "val_mse = mean_squared_error(y_val, y_val_preds).round(5)\n",
    "train_r2 = r2_score(y_train, y_train_preds).round(5)\n",
    "val_r2 = r2_score(y_val, y_val_preds).round(5)\n",
    "\n",
    "print(\"=== Model Performance ===\")\n",
    "print(f\"Train MSE: {proj_utils_plots.beautify(train_mse)}, Train R2: {proj_utils_plots.beautify(train_r2)}\")\n",
    "print(f\"Validation MSE: {proj_utils_plots.beautify(val_mse)}, Validation R2: {proj_utils_plots.beautify(val_r2)}\")"
   ],
   "id": "36fce385b777a442",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "string_to_log = f'=== Model Performance === \\n Train MSE: {train_mse}, Train R2: {train_r2} \\n Validation MSE: {val_mse}, Validation R2: {val_r2}'\n",
    "proj_utils.save_file('metrics', 'validation_metrics.txt', proj_configs.PATH_OUT_MODELS, string_to_log)"
   ],
   "id": "bcc70e6544b2a605",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d05ca7a227112434",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
